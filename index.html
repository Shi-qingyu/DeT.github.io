<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Benchmarking and Improving Video Diffusion Transformers For Motion Transfer">
  <meta name="keywords" content="DiT, Motion Transfer">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Decouple and Track: Benchmarking and Improving Video Diffusion Transformers For Motion Transfer</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/Shi-qingyu">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Decouple and Track: Benchmarking and Improving Video Diffusion Transformers For Motion Transfer</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://github.com/Shi-qingyu">Qingyu Shi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://jianzongwu.github.io/">Jianzong Wu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://noyii.github.io/">Jinbin Bai</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhangzjn.github.io/">Jiangning Zhang</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="http://luqi.info/">Lu Qi</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://lxtgh.github.io/">Xiangtai Li</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=T4gqdPkAAAAJ&hl=zh-CN">Yunhai Tong</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Peking University,</span>
            <span class="author-block"><sup>2</sup>Nanyang Technological University,</span>
            <span class="author-block"><sup>3</sup>National University of Singapore,</span>
            <span class="author-block"><sup>4</sup>Zhejiang University,</span>
            <span class="author-block"><sup>5</sup>UC Merced</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2503.17350"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Arxiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.17350"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Shi-qingyu/DeT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (Coming Soon)</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="det">DeT</span> transfers the motion in the source video to newly generated videos.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The motion transfer task involves transferring motion from a source video to newly generated videos, 
            requiring the model to decouple motion from appearance. 
          </p>
          <p>
            Previous diffusion-based methods primarily rely on separate spatial and temporal attention mechanisms within 3D U-Net.
            In contrast, state-of-the-art Diffusion Transformer (DiT) models use 3D full attention, 
            which does not explicitly separate temporal and spatial information. 
            Thus, the interaction between spatial and temporal dimensions makes decoupling motion and appearance more challenging for DiT models.
          </p>
          <p>
            In this paper, we propose DeT, a method that adapts DiT models to improve motion transfer ability. 
            Our approach introduces a simple yet effective temporal kernel to smooth DiT features along the temporal dimension, 
            facilitating the decoupling of foreground motion from background appearance. 
            Meanwhile, the temporal kernel effectively captures temporal variations in DiT features, which are closely related to motion. 
            Moreover, we introduce explicit supervision along trajectories in the latent feature space to further enhance motion consistency. 
            Additionally, we present MTBench, a general and challenging benchmark for motion transfer. 
            We also introduce a hybrid motion fidelity metric that considers both the global and local similarity of motion. 
            Therefore, our work provides a more comprehensive evaluation than previous works.
            Extensive experiments on MTBench demonstrate that DeT achieves the best trade-off between motion fidelity and edit fidelity.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- DeT. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">DeT</h2>
        <!-- Method -->
        <div class="has-text-centered">
          <img src="static/images/method.png" alt="Motivation & DeT" style="max-width: 100%; height: auto;">
        </div>
        <div class="content has-text-justified">
          <p>
            We first implement two existing methods compatible for DiT models as baselines.
            Through toy experiments shown in Fig. (2) (a), 
            we observe that these baselines struggle to control the background through text. 
            By visualizing the outputs of 3D full attention in Fig. (1), 
            we find that foreground and background features are difficult to distinguish, 
            indicating that foreground motion and background appearance are not correctly decoupled. 
            We argue that this is due to the temporal inconsistencies of the background feature in the denoising process, as shown in Fig. (2) (a). 
            This makes it challenging to differentiate between foreground and background in certain frames.
          </p>
          <p>
            To encourage the model to decouple background appearance from foreground motion, 
            we smooth the feature in 3D full attention along the temporal dimension. 
            This facilitates a clearer separation between foreground and background. 
          </p>
          <p>
          <div class="has-text-centered">
            <img src="static/images/attention_map.png" alt="Attention" style="max-width: 100%; height: auto;">
          </div>  
            On the other hand, to accurately learn motion, 
            we examine the 3D attention map and find that significant attention scores appear primarily along the diagonal for adjacent frames.
            This observation motivates us to adopt a temporal 1D kernel to learn temporal information while simultaneously decoupling foreground appearance. 
            The temporal kernel also serves as an effective method for smoothing features across time. 
            By applying a trainable shared temporal kernel along the temporal dimension, 
            we achieve motion alignment while ensuring the decoupling of both foreground and background appearance. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- DeT. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">MTBench</h2>
        <!-- Method -->
        <div class="has-text-centered">
          <img src="static/images/mtbench.png" alt="MTBench" style="max-width: 100%; height: auto;">
        </div>
        <div class="content has-text-justified">
          <p>
            We construct a more general benchmark based on the DAVIS and YouTube-VOS datasets, 
            which includes 100 high-quality videos with 500 evaluation prompts. 
            We use the Qwen2.5-VL-7B to annotate captions for the videos. 
            Based on each caption, we generate five evaluation text prompts using the Qwen2.5-14B, 
            swapping the foreground and background while keeping the verb unchanged. Additionally, 
            we use the SAM and CoTracker to annotate masks and trajectories for the foreground in the videos. 
          </p>
          <p>
            We then perform automatic K-means clustering of all trajectories based on the silhouette coefficient. 
            The motion in each video is categorized into easy, medium, and hard levels based on the number of clusters. 
            We visualize the motion distribution of evaluation prompts Fig (a) and present examples of the difficulty division in Fig (b).
            We also visualize the difficulty distribution in Fig (c).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Results. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <div class="has-text-centered mb-6">
          <h2 class="title is-3">Experimental Results</h2>
        </div>

        <!-- Quantitative Results. -->
        <h3 class="title is-4">Quantitative Results</h3>
        <div class="content has-text-justified">
          <p>
            Our method with HunyuanVideo achieves the highest Motion Fidelity. 
            While MOFT is slightly higher in edit fidelity, our method achieves the best balance between edit and motion fidelity.
            Additionally, we adapt MotionInversion, DreamBooth, and DMT to the DiT model, 
            all of which underperform compared to our method across all metrics. 
            Furthermore, in previous works, motion fidelity decreases as difficulty increases, while edit fidelity improves. 
            This indicates that our benchmark difficulty division is reasonable. 
            Notably, our method achieves the highest score on the medium difficulty level, 
            which we attribute to its significant performance improvement in this level.
          </p>
        </div>
        <div class="has-text-centered">
          <img src="static/images/quantitative.png" alt="Attention" style="max-width: 100%; height: auto;">
        </div>
        <br/>
        <!-- Quantitative Results. -->

        <!-- Qualitative Results. -->
        <h3 class="title is-4">Qualitative Results</h3>
        <div class="content has-text-justified">
          <p>
            Our method accurately transfers the motion from the source video without overfitting to the appearance. 
            This enables flexible text control over both the foreground and background. 
            Additionally, our method supports motion transfer across different categories, 
            such as from a human to a panda or from a train to a boat.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/qual.mp4"
                    type="video/mp4">
          </video>
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/qual_2.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Qualitative Results. -->

        <!-- Comparison. -->
        <h3 class="title is-4">Comparison</h3>
        <div class="content has-text-justified">
          <p>
            Compared with other motion transfer methods, our method accurately transfers motion patterns while allowing flexible text-based control over both the foreground and background appearance.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/comparison.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Comparison. -->

      </div>
    </div>
  </div>
</section>


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
